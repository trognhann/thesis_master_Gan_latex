\documentclass[a4paper,12pt,oneside]{book}%extreport
\usepackage{times} 
% \usepackage{multibib}
\usepackage{lipsum}
\usepackage{appendix}
\usepackage[shortlabels]{enumitem}
\usepackage{booktabs}
\usepackage{rotating} % Rotating table
\usepackage{hhline}
\usepackage{colortbl}
\usepackage{makecell}
\usepackage{afterpage}
\usepackage{mathtools} %Fixes/improves amsmath
\usepackage{setspace}
\usepackage[utf8]{vietnam} 
\usepackage{amstext, amsmath,latexsym,amsbsy,amssymb, amssymb,amsthm,amsfonts,multicol, nccmath}
\usepackage[left=3cm,right=2cm,top=2.5cm,bottom=3cm,footskip=40pt]{geometry}
\usepackage{pdflscape}
\usepackage{apacite}
\usepackage{tikz}
\usepackage{array}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage{subfig}
\usetikzlibrary{calc}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{array,color,colortbl}
\setcounter{secnumdepth}{4}
 %\usepackage[square,numbers]{natbib}
\usepackage[square, comma, numbers, sort&compress]{natbib}
\setlength{\parindent}{1cm}
\usepackage{color}
\usepackage{indentfirst}
\usepackage{exscale,eucal}
\usepackage{fancyhdr}
\usepackage{fncychap}
\usepackage[chapter]{algorithm}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{tabularx,multicol,multirow,longtable}
\usepackage{etoolbox}
\usepackage{tikz}	\usetikzlibrary{calc,arrows,decorations.pathmorphing,backgrounds,positioning,fit,shapes,decorations.shapes, shapes.geometric, decorations.pathreplacing, decorations.text}
 
	\tikzstyle{block} = [draw,rectangle,thick,minimum height=2em,minimum width=2em,drop shadow,fill=blue!50]
	\tikzstyle{sum} = [draw,circle,inner sep=0mm,minimum size=2mm]
	\tikzstyle{connector} = [->,thick]
	\tikzstyle{line} = [very thick]
	\tikzstyle{branch} = [circle,inner sep=0pt,minimum size=1mm,fill=black,draw=black]
	\tikzstyle{axes} = [->,>=stealth',semithick]
	\tikzstyle{important line} = [very thick,draw=red]
	\tikzstyle{important text} = [rounded corners,fill=red!10,inner sep=1ex]
\usepackage{circuitikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage[numbers]{natbib}
\usepackage{titlesec}
\usepackage[labelsep=period]{caption}
% \usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}

\DeclareCaptionFormat{myformat}{\fontsize{13}{15}\selectfont#1#2#3}
\captionsetup{format=myformat}
\usepackage{titletoc}
\titlelabel{\thetitle.\,\,}
\titleformat{\chapter}[display] 
  {\fontsize{14}{16}\selectfont\bfseries\centering}
  {\MakeUppercase{\chaptertitlename}\ \thechapter}{0pt}{\fontsize{14}{16}\selectfont\MakeUppercase}
\titlespacing{\chapter}{0pt}{0pt}{40pt} 
\titleformat*{\section}{\fontsize{14}{14}\selectfont\bfseries}
\titleformat*{\subsection}{\fontsize{14}{14}\selectfont\bfseries\slshape}
\titleformat*{\subsubsection}{\fontsize{14}{14}\slshape}
\titleformat*{\paragraph}{\large\bfseries}
\titleformat*{\subparagraph}{\large\bfseries}
\graphicspath {{figures/}}

\titlespacing\section{0pt}{6pt plus 4pt minus 2pt}{1pt plus 2pt minus 2pt} 
\titlespacing\subsection{0pt}{6pt plus 4pt minus 2pt}{1pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{6pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsubsection{0pt}{6pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}

\usepackage[subfigure]{tocloft} 

\cftsetpnumwidth{20pt}
\titlecontents{chapter}
  [0pt]
  {}
  {\fontsize{13.5}{14}\selectfont\MakeUppercase{\chaptername}\ \thecontentslabel.\,\,}
  {}
  {\cftdotfill{\cftdotsep}\contentspage} 
	\renewcommand{\cftsecaftersnum}{.}%
	\renewcommand{\cftsubsecaftersnum}{.}%

\usetikzlibrary{calc}
\newtheorem{definition}{\bf Định nghĩa}[chapter]
\newtheorem{theorem}{\bf Định lý}[chapter]
\newtheorem{lemma}{\bf Bổ đề}[chapter]

\renewcommand{\cftfigfont}{Hình~}
\renewcommand{\cfttabfont}{Bảng~ }
\usepackage{float}
\floatname{algorithm}{Thuật toán}
\makeatletter
\renewcommand\@biblabel[1]{[#1]}
\renewcommand\harvardyearleft{\unskip~(}
\renewcommand\harvardyearright{\unskip )}
\makeatother
\renewcommand{\baselinestretch}{1.4}
\flushbottom
\renewcommand*{\bibfont}{\fontsize{13}{16}\selectfont}

\usepackage[hidelinks, unicode]{hyperref}

\begin{document}
\fontsize{13}{15.5}\selectfont

\pagestyle{empty}
\input{cover/cover}
\newpage

\def\baselinestretch{1.3}
\pagestyle{plain}
\pagenumbering{gobble}
\input{cover/acknowledgement.tex}
\newpage
\input{cover/assurance.tex} 
\newpage
\input{cover/abstract_vn.tex}
\newpage
\input{cover/abstract_en.tex}
\newpage
\pagenumbering{roman}


\setlength{\parindent}{1cm}
\setlength{\parskip}{0.6ex}

\fontsize{13}{16}\selectfont
\renewcommand{\contentsname}{\vspace{-70pt}\centerline{\fontsize{14}{16}\selectfont\MakeUppercase{Mục lục}}}
\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Mục lục}
\tableofcontents
\clearpage
% \input{cover/tomtat}
\newpage
\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Danh mục hình vẽ}
\renewcommand{\listfigurename}{\vspace{-70pt}\centerline{\fontsize{14}{16}\selectfont{\MakeUppercase{Danh mục   hình vẽ}}}}
\listoffigures
\fontsize{13}{16}\selectfont
\newpage
\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Danh mục bảng biểu}
\renewcommand{\listtablename}{\vspace{-70pt}\centerline{\fontsize{14}{16}\selectfont{\MakeUppercase{Danh mục bảng biểu}}}}
\listoftables
\fontsize{13}{16}\selectfont
\newpage

\clearpage
\input{cover/symbol.tex}
\newpage

\pagenumbering{arabic}
\pagestyle{plain}

\input{chapter/chap0_intro.tex}
\newpage
\input{chapter/chap1_Prologue.tex}
\newpage
\input{chapter/chap2_main.tex}
\newpage
% \input{chapter/Chap3_Main}
% \newpage
% \input{chapter/Chap4_Ketluan}
% \newpage

\appendix
% \input{cover/phuluc}
\def\baselinestretch{1}
\vspace{-2cm}
\renewcommand{\bibname}{Tài liệu tham khảo}
\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{{TÀI LIỆU THAM KHẢO}}
\renewcommand{\refname}{Literary works}
\begin{thebibliography}{xx}
	\section*{Tiếng Anh}
	\vspace{0.3cm}

\harvarditem{{Al-Waisy et al.}}{2019}{AlWaisy2019}
Al-Waisy, A. S., et al. (2019), ``Multi-Scale Inception Based Super-Resolution Using Deep Learning Approach'', {\em Electronics}, Vol. 8(8), pp. 892.

\harvarditem{{Arjovsky et al.}}{2017}{Arjovsky2017}
Arjovsky, M., Chintala, S., Bottou, L. (2017), ``Wasserstein GAN'', {\em arXiv preprint arXiv:1701.07875}.

\harvarditem{{Arnaud58}}{n.d.}{Selfie2Anime}
Arnaud58 (n.d.), ``selfie2anime - Kaggle'', {\em https://www.kaggle.com/datasets/arnaud58/selfie2anime}.

\harvarditem{{Ashraf et al.}}{2023}{Ashraf2023}
Ashraf, S. M. N., Mamun, M. A., Abdullah, H. M., Alam, M. G. R. (2023), ``SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models for Multi-Label Chest X-Ray Classification'', {\em 2023 International Conference on Computer and Information Technology (ICCIT)}, arXiv:2311.07750.

\harvarditem{{Aurora Solar}}{n.d.}{AuroraGANvsDiff}
Aurora Solar (n.d.), ``GANs vs. Diffusion Models: Putting AI to the test'', {\em https://aurorasolar.com/blog/putting-ai-to-the-test-generative-adversarial-networks-vs-diffusion-models/}.

\harvarditem{{Basha et al.}}{2020}{Basha2020}
Basha, S. H. S., Dubey, S. R., Pulabaigari, V., Mukherjee, S. (2020), ``Impact of Fully Connected Layers on Performance of Convolutional Neural Networks for Image Classification'', {\em Neurocomputing}, Vol. 378, pp. 178-189.

\harvarditem{{Brock et al.}}{2018}{Brock2018}
Brock, A., Donahue, J., Simonyan, K. (2018), ``Large Scale GAN Training for High Fidelity Natural Image Synthesis'', {\em International Conference on Learning Representations (ICLR)}.

\harvarditem{{Cao et al.}}{2025}{Cao2025}
Cao, Y., et al. (2025), ``Generative Artificial Intelligence in Robotic Manipulation: A Survey'', {\em arXiv preprint arXiv:2503.03464}.

\harvarditem{{Chen et al.}}{2020}{Chen2020}
Chen, J., Liu, G., Chen, X. (2020), ``AnimeGAN: A Novel Lightweight GAN for Photo Animation'', {\em Artificial Intelligence Algorithms and Applications}, Springer, Singapore, pp. 242-256.

\harvarditem{{Chen \& Liu}}{2020}{ChenLiu2020}
Chen, X., Liu, G. (2020), ``AnimeGANv2'', {\em https://tachibanayoshino.github.io/AnimeGANv2/}.

\harvarditem{{Gholamalinezhad \& Khosravi}}{2022}{Gholamalinezhad2022}
Gholamalinezhad, H., Khosravi, H. (2022), ``A Comparison of Pooling Methods for Convolutional Neural Networks'', {\em Applied Sciences}, Vol. 12(17), pp. 8643.

\harvarditem{{Girshick et al.}}{2014}{Girshick2014}
Girshick, R., Donahue, J., Darrell, T., Malik, J. (2014), ``Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation'', {\em Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}.

\harvarditem{{Go et al.}}{2025}{Go2025}
Go, H., et al. (2025), ``Generative AI in Depth: A Survey of Recent Advances, Model Variants, and Real-World Applications'', {\em arXiv preprint arXiv:2510.21887}.

\harvarditem{{Goodfellow et al.}}{2014}{Goodfellow2014}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y. (2014), ``Generative Adversarial Nets'', {\em Advances in Neural Information Processing Systems (NIPS)}, pp. 2672-2680.

\harvarditem{{Google Developers}}{n.d.}{GoogleGANProblems}
Google Developers (n.d.), ``Common Problems | Machine Learning'', {\em https://developers.google.com/machine-learning/gan/problems}.

\harvarditem{{Gulrajani et al.}}{2017}{Gulrajani2017}
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A. C. (2017), ``Improved Training of Wasserstein GANs'', {\em Advances in Neural Information Processing Systems (NIPS)}.

\harvarditem{{He et al.}}{2017}{He2017}
He, K., Gkioxari, G., Dollár, P., Girshick, R. (2017), ``Mask R-CNN'', {\em arXiv preprint arXiv:1703.06870}.

\harvarditem{{Heusel et al.}}{2017}{Heusel2017}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S. (2017), ``GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium'', {\em Advances in Neural Information Processing Systems (NIPS)}.

\harvarditem{{Hippocampus's Garden}}{2021}{Hippocampus2021}
Hippocampus's Garden (2021), ``Awesome StyleGAN Applications'', {\em https://hippocampus-garden.com/stylegans/}.

\harvarditem{{Hui}}{2018}{Hui2018}
Hui, J. (2018), ``GAN — Why it is so hard to train Generative Adversarial Networks!'', {\em Medium}, https://jonathan-hui.medium.com/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b.

\harvarditem{{Hussain et al.}}{2018}{Hussain2018}
Hussain, M., Bird, J. J., Faria, D. R. (2018), ``A Study on CNN Transfer Learning for Image Classification'', {\em Advances in Intelligent Systems and Computing}, Vol. 840, Springer, pp. 191–202.

\harvarditem{{Jabbar et al.}}{2021}{Jabbar2021}
Jabbar, A., Li, X., Omar, B. (2021), ``Generative Adversarial Networks (GANs): Challenges, Solutions, and Future Directions'', {\em ACM Computing Surveys (CSUR)}, Vol. 54(8), pp. 1-29.

\harvarditem{{Karras et al.}}{2019}{Karras2019}
Karras, T., Laine, S., Aila, T. (2019), ``A Style-Based Generator Architecture for Generative Adversarial Networks'', {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}.

\harvarditem{{Karras et al.}}{2020}{Karras2020}
Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T. (2020), ``Analyzing and Improving the Image Quality of StyleGAN'', {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}.

\harvarditem{{Lambdanalytique}}{2022}{LambdaStyleGAN}
Lambdanalytique (2022), ``StyleGAN vs StyleGAN2 vs StyleGAN2-ADA vs StyleGAN3'', {\em https://lambdanalytique.com/2022/07/01/stylegan-vs-stylegan2-vs-stylegan2-ada-vs-stylegan3/}.

\harvarditem{{Lin}}{2025}{Lin2025}
Lin, F. (2025), ``Vision Language Models: A Survey of 26K Papers'', {\em arXiv preprint arXiv:2510.09586}.

\harvarditem{{Liu et al.}}{2024}{Liu2024}
Liu, G., Chen, X., Gao, Z. (2024), ``A Novel Double-Tail Generative Adversarial Network for Fast Photo Animation'', {\em IEICE Transactions on Information and Systems}, Vol. E107.D(1), pp. 72-82.

\harvarditem{{Liu et al.}}{2024}{AnimeGANv3}
Liu, G., Chen, X., Gao, Z. (2024), ``AnimeGANv3: A Novel Double-Tail Generative Adversarial Network for Fast Photo Animation'', {\em https://tachibanayoshino.github.io/AnimeGANv3/}.

\harvarditem{{Liu et al.}}{2017}{Liu2017}
Liu, M.-Y., Breuel, T., Kautz, J. (2017), ``Unsupervised Image-to-Image Translation Networks'', {\em Advances in Neural Information Processing Systems (NIPS)}.

\harvarditem{{Lo et al.}}{2024}{Lo2024}
Lo, S.-L., Cheng, H.-Y., Yu, C.-C. (2024), ``Feature Weighted Cycle Generative Adversarial Network with Facial Landmark Recognition and Perceptual Color Distance for Enhanced Face Animation Generation'', {\em Electronics}, Vol. 13(23), pp. 4761.

\harvarditem{{Lu et al.}}{2024}{Lu2024}
Lu, Z., Zhou, Y., Chen, A. (2024), ``Enhancing Photo Animation: Augmented Stylistic Modules and Prior Knowledge Integration'', {\em Proceedings of the Asian Conference on Computer Vision (ACCV)}, pp. 1470–1485.

\harvarditem{{Machine Learning Mastery}}{n.d.}{MLMasteryFID}
Machine Learning Mastery (n.d.), ``How to Implement the Frechet Inception Distance (FID) for Evaluating GANs'', {\em https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/}.

\harvarditem{{Machine Learning Mastery}}{n.d.}{MLMasteryIS}
Machine Learning Mastery (n.d.), ``How to Implement the Inception Score (IS) for Evaluating GANs'', {\em https://machinelearningmastery.com/how-to-implement-the-inception-score-from-scratch-for-evaluating-generated-images/}.

\harvarditem{{Miyato et al.}}{2018}{Miyato2018}
Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y. (2018), ``Spectral Normalization for Generative Adversarial Networks'', {\em International Conference on Learning Representations (ICLR)}.

\harvarditem{{Neptune.ai}}{n.d.}{NeptuneGANFailure}
Neptune.ai (n.d.), ``GANs Failure Modes: How to Identify and Monitor Them'', {\em https://neptune.ai/blog/gan-failure-modes}.

\harvarditem{{Paperspace}}{n.d.}{PaperspaceStyleGAN}
Paperspace (n.d.), ``The Evolution of StyleGAN'', {\em https://blog.paperspace.com/evolution-of-stylegan/}.

\harvarditem{{Radford et al.}}{2015}{Radford2015}
Radford, A., Metz, L., Chintala, S. (2015), ``Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks'', {\em arXiv preprint arXiv:1511.06951}.

\harvarditem{{Rao}}{n.d.}{RaoModeCollapse}
Rao, N. (n.d.), ``Analyzing the Mode Collapse Problem in GANS'', {\em https://raonikitha.github.io/files/academic-posts/ModeCollapseProblem.pdf}.

\harvarditem{{Saad et al.}}{2024}{Saad2024}
Saad, M. M., O'Reilly, R., Rehmani, M. H. (2024), ``A survey on training challenges in generative adversarial networks for biomedical image analysis'', {\em Artificial Intelligence Review}, Vol. 57(2).

\harvarditem{{Salimans et al.}}{2016}{Salimans2016}
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X. (2016), ``Improved Techniques for Training GANs'', {\em Advances in Neural Information Processing Systems (NIPS)}.

\harvarditem{{Sapien}}{n.d.}{SapienGANvsDiff}
Sapien (n.d.), ``GANs vs. Diffusion Models: In-Depth Comparison and Analysis'', {\em https://www.sapien.io/blog/gans-vs-diffusion-models-a-comparative-analysis}.

\harvarditem{{Sauer et al.}}{2025}{Sauer2025}
Sauer, A., et al. (2025), ``The GAN is dead; long live the GAN! A Modern Baseline GAN'', {\em arXiv preprint arXiv:2501.05441}.

\harvarditem{{Stanford University}}{n.d.}{CS231n}
Stanford University (n.d.), ``CS231n: Deep Learning for Computer Vision'', {\em http://vision.stanford.edu/teaching/cs231n/}.

\harvarditem{{Szegedy et al.}}{2017}{Szegedy2017}
Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A. A. (2017), ``Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning'', {\em Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence}, pp. 4278-4284.

\harvarditem{{Towards Data Science}}{n.d.}{TDSWGAN}
Towards Data Science (n.d.), ``Demystified: Wasserstein GANs (WGAN)'', {\em https://towardsdatascience.com/demystified-wasserstein-gans-wgan-f835324899f4/}.

\harvarditem{{Tsang}}{n.d.}{TsangReview}
Tsang, S.-H. (n.d.), ``Review — Improved Techniques for Training GANs'', {\em Medium}, https://sh-tsang.medium.com/review-improved-techniques-for-training-gans-348a13900aee.

\harvarditem{{Hasan, S. M. K., & Linte, C. A.}}{2019}{Hasan2019}
Hasan, S. M. K., \& Linte, C. A. (2019), ``U-NetPlus: A Modified Encoder-Decoder U-Net Architecture for Semantic and Instance Segmentation of Surgical Instruments from Laparoscopic Images'', {\em https://arxiv.org/pdf/1902.08994}.

\harvarditem{{Wang et al.}}{n.d.}{WangLAST}
Wang, Y., Wen, R., Ishii, H., Ohya, J. (n.d.), ``LAST: Utilizing Synthetic Image Style Transfer to Tackle Domain Shift in Aerial Image Segmentation'', {\em https://arxiv.org/pdf/2206.06666}.

\harvarditem{{Weng}}{2017}{Weng2017}
Weng, L. (2017), ``From GAN to WGAN'', {\em Lil'Log}, https://lilianweng.github.io/posts/2017-08-20-gan/.

\harvarditem{{Weng}}{2018}{Weng2018}
Weng, L. (2018), ``Attention? Attention!'', {\em Lil'Log}, https://lilianweng.github.io/posts/2018-06-24-attention/.

\harvarditem{{Wikipedia}}{n.d.}{WikiAlexNet}
Wikipedia (n.d.), ``AlexNet'', {\em https://en.wikipedia.org/wiki/AlexNet}.

\harvarditem{{Wikipedia}}{n.d.}{WikiCNN}
Wikipedia (n.d.), ``Convolutional neural network'', {\em https://en.wikipedia.org/wiki/Convolutional\_neural\_network}.

\harvarditem{{Wikipedia}}{n.d.}{WikiFID}
Wikipedia (n.d.), ``Fréchet inception distance'', {\em https://en.wikipedia.org/wiki/Fr\%C3\%A9chet\_inception\_distance}.

\harvarditem{{Wikipedia}}{n.d.}{WikiInceptionScore}
Wikipedia (n.d.), ``Inception score'', {\em https://en.wikipedia.org/wiki/Inception\_score}.

\harvarditem{{Yang et al.}}{2022}{Yang2022}
Yang, S., Jiang, L., Liu, Z., Loy, C. C. (2022), ``Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer'', {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 11728-11737.

\harvarditem{{Yang et al.}}{2022}{YangDiffusion2022}
Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Shao, Y., Zhang, W., Cui, B., Yang, M.-H. (2022), ``A Survey on Generative Diffusion Models'', {\em arXiv preprint arXiv:2209.02646}.

\harvarditem{{Zhang \& Tang}}{2025}{Zhang2025}
Zhang, T., Tang, H. (2025), ``Style Transfer: A Decade Survey'', {\em arXiv preprint arXiv:2506.19278}.

\harvarditem{{Zhang et al.}}{2019}{Zhang2019}
Zhang, H., Goodfellow, I., Metaxas, D., Odena, A. (2019), ``Self-Attention Generative Adversarial Networks'', {\em International Conference on Machine Learning (ICML)}.

\harvarditem{{Zhou et al.}}{2018}{Zhou2018}
Zhou, Z., Siddiquee, M. R., Tajbakhsh, N., Liang, J. (2018), ``UNet++: A Nested U-Net Architecture for Medical Image Segmentation'', {\em Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support}, Vol. 11045, Springer, Cham, pp. 3-11.

\harvarditem{{Zhu et al.}}{2017}{Zhu2017}
Zhu, J.-Y., Park, T., Isola, P., Efros, A. A. (2017), ``Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks'', {\em Proceedings of the IEEE International Conference on Computer Vision (ICCV)}.

\harvarditem{{Zouaoui}}{n.d.}{ZouaouiStyleGAN}
Zouaoui, A. (n.d.), ``StyleGAN: Explained'', {\em Medium}, https://medium.com/@arijzouaoui/stylegan-explained-3297b4bb813a.

\end{thebibliography}

\end{document}
